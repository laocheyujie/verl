# 权重转换

> https://verl.readthedocs.io/en/latest/advance/checkpoint.html#convert-fsdp-and-megatron-checkpoints-to-huggingface-format-model

usage:

```bash
python -m verl.model_merger merge [-h] --backend {fsdp,megatron} [--local_dir LOCAL_DIR] [--tie-word-embedding] [--is-value-model] [--use_cpu_initialization] [--target_dir TARGET_DIR]
[--hf_upload_path HF_UPLOAD_PATH] [--private]
```

options:

- -h, --help show this help message and exit
- --backend {fsdp,megatron}:The backend of the model
- --local_dir LOCAL_DIR: Path to the saved model checkpoints
- --tie-word-embedding Whether to tie word embedding weights (currently only Megatron supported)
- --is-value-model Whether the model is a value model (currently only Megatron supported)
- --use_cpu_initialization: Whether to use CPU initialization for the model. This is useful for large models that cannot fit into GPU memory during initialization.
- --target_dir TARGET_DIR: Directory to save the merged huggingface model
- --hf_upload_path HF_UPLOAD_PATH: Hugging Face repository ID to upload the model
- --private Whether to upload the model to a private Hugging Face repository

## Megatron to Safetensors

### 单节点

```bash
python -m verl.model_merger merge \
    --backend megatron \
    --tie-word-embedding \
    --local_dir checkpoints/verl_megatron_gsm8k_examples/qwen2_5_0b5_megatron_saveload/global_step_1/actor \
    --target_dir /path/to/merged_hf_model
```

### 多节点

```bash
torchrun --nproc_per_node 1 --nnodes 8 --node_rank ${RANK} -m verl.model_merger merge \
    --backend megatron \
    --tie-word-embedding \
    --local_dir checkpoints/verl_megatron_gsm8k_examples/qwen2_5_0b5_megatron_saveload/global_step_1/actor \
    --target_dir /path/to/merged_hf_model
```

## FSDP to Megatron

.pt -> .safetensor

### 新版本

对于有 `fsdp_config.json` 的新版 FSPD

```bash
python -m verl.model_merger merge \
    --backend fsdp \
    --local_dir checkpoints/verl_fsdp_gsm8k_examples/qwen2_5_0b5_fsdp_saveload/global_step_1/actor \
    --target_dir /path/to/merged_hf_model
```

### 旧版本

对于没有 `fsdp_config.json` 的旧版 FSDP，使用 `verl/scripts/legacy_model_merger.py` 进行转换

## Safetensors to Megatron

### 单节点

```bash
python scripts/converter_hf_to_mcore.py \
    --hf_model_path Qwen/Qwen1.5-MoE-A2.7B-Chat \
    --output_path /mnt/disk/Qwen/Qwen1.5-MoE-A2.7B-Chat \
    --use_cpu_initialization    # Only work for MoE models
```

### 多节点

```bash
torchrun --nproc_per_node 1 --nnodes 8 --node_rank ${RANK} scripts/converter_hf_to_mcore.py \
    --hf_model_path deepseek-ai/DeepSeek-V3 \
    --output_path /mnt/disk/deepseek-ai/DeepSeek-V3 \
    --use_cpu_initialization    # Only work for MoE models
```
